{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "167516"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"data/wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "\n",
    "FILE_PATH = \"data/wonderland.txt\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "# read the data\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: \n",
      " 0123456789abcdefghijklmnopqrstuvwxyz﻿\n",
      "Number of characters: 158596\n",
      "Number of unique characters: 39\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text)))\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "int2char = {i: c for i, c in enumerate(vocab)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "encoded_text = np.array([char2int[c] for c in text])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 ﻿\n",
      "27 p\n",
      "29 r\n",
      "26 o\n",
      "21 j\n",
      "16 e\n",
      "14 c\n",
      "31 t\n"
     ]
    }
   ],
   "source": [
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "\n",
      "almost no restrictions whatsoever  you may copy it give it away\n",
      " or\n",
      "\n",
      "reuse it under the terms of the project gutenberg license included\n",
      "\n",
      "with this ebook or online at wwwgutenbergorg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title alices adventures in wonderland\n",
      "\n",
      "\n",
      "\n",
      "author lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "posting date \n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "    # example :\n",
    "    # sequence_length is 10\n",
    "    # sample is \"python is a great pro\" (21 length)\n",
    "    # ds will equal to ('python is ', 'a') encoded as integers\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2):\n",
    "        # first (input_, target) will be ('ython is a', ' ')\n",
    "        # second (input_, target) will be ('thon is a ', 'g')\n",
    "        # third (input_, target) will be ('hon is a g', 'r')\n",
    "        # and so on\n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "        # extend the dataset with these samples by concatenate() method\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "# prepare inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # onehot encode the inputs and the targets\n",
    "    # Example:\n",
    "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
    "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of\n",
      "Target:  \n",
      "Input shape: (100, 39)\n",
      "Target shape: (39,)\n",
      "================================================== \n",
      "\n",
      "Input: project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "\n",
      "\n",
      "this ebook is for the use of \n",
      "Target: a\n",
      "Input shape: (100, 39)\n",
      "Target shape: (39,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 100, 256)          303104    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 256)          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 39)                10023     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 838,439\n",
      "Trainable params: 838,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/619 [==================>...........] - ETA: 6:20:56 - loss: 0.9010 - accuracy: 0.7177"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "# train the model\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
    "# save the model\n",
    "model.save(model_weights_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}